import numpy as np
import math
from munch import munchify
import torch
from lero_control_gym.controllers.ImitationMPC.mpc import mpc
from lero_control_gym.controllers.base_controller import BaseController


class MPC(BaseController):

    class QuadDynamics(torch.nn.Module):

        class Propeller(torch.nn.Module):
            def __init__(self, prop_dia, prop_pitch, thrust_unit='N'):
                super().__init__()
                self.dtype = torch.float64
                # Define the propeller diameter as a parameter with requires_grad = False,
                # meaning that its value cannot be updated during training.
                self.prop_dia = torch.nn.Parameter(torch.tensor(prop_dia), requires_grad=False)
                # Define the propeller pitch as a parameter with requires_grad = False,
                # meaning that its value cannot be updated during training.
                self.prop_pitch = torch.nn.Parameter(torch.tensor(prop_pitch), requires_grad=False)
                # Store the unit in which thrust is to be measured (N for Newtons or Kg for Kgf).
                self.thrust_unit = thrust_unit
                # Define the speed of the propeller as a parameter with requires_grad = False,
                # meaning that its value cannot be updated during training.
                self.speed = torch.tensor(0.0 , dtype=self.dtype)
                # Define the thrust generated by the propeller as a parameter with requires_grad = False,
                # meaning that its value cannot be updated during training.
                self.thrust = torch.tensor(0.0 , dtype=self.dtype)

            def set_speed(self, speed):
                # Set the speed of the propeller to the input value.
                self.speed =  torch.tensor(0.0 , dtype=self.dtype)
                # Compute the thrust generated by the propeller using the formula from
                # http://www.electricrcaircraftguy.com/2013/09/propeller-static-dynamic-thrust-equation.html
                self.thrust = 4.392e-8 * self.speed * (self.prop_dia ** 3.5) / (torch.sqrt(self.prop_pitch))
                self.thrust = self.thrust * (4.23e-4 * self.speed * self.prop_pitch)

                # If the unit of thrust is Kgf, convert it to Newtons.
                if self.thrust_unit == 'Kg':
                    self.thrust = self.thrust * 0.101972

        QUADCOPTER_PARAMETERS = {
            # Initial position of the quadcopter in 3D space, represented as [x, y, z]
            "position": [0, 0, 0],
            # Initial orientation of the quadcopter in 3D space, represented as [roll, pitch, yaw]
            "orientation": [0, 0, 0],
            # Length of the quadcopter arms
            "L": 0.3,
            # Radius of the quadcopter
            "r": 0.1,
            # Dimension of the propellers, represented as [diameter, pitch]
            "prop_size": [10, 4.5],
            # Total weight of the quadcopter
            "weight": 1.2
        }

        def __init__(self, Quad=QUADCOPTER_PARAMETERS, gravity=9.81, b=0.0245):
            super().__init__()
            """
            Initialize the QuadcopterModel class with the given parameters.

            Parameters:
            Quad (dict, optional): Dictionary of quadcopter parameters. Default is `QUADCOPTER_PARAMETERS`.
            gravity (float, optional): Acceleration due to gravity. Default is 9.81 m/s^2.
            b (float, optional): Air drag coefficient. Default is 0.0245.
            """
            # Store the quadcopter parameters
            self.quads = Quad
            self.g = torch.tensor(gravity, dtype=torch.float64)
            self.b = torch.tensor(b, dtype=torch.float64)
            self.maxRPM = 10000
            self.state = torch.zeros(12, requires_grad=True)
            self.action = torch.zeros(4, requires_grad=True)
            # Initialize the four propellers of the quadcopter
            self.Prop1 = self.Propeller(self.quads['prop_size'][0], self.quads['prop_size'][1])
            self.Prop2 = self.Propeller(self.quads['prop_size'][0], self.quads['prop_size'][1])
            self.Prop3 = self.Propeller(self.quads['prop_size'][0], self.quads['prop_size'][1])
            self.Prop4 = self.Propeller(self.quads['prop_size'][0], self.quads['prop_size'][1])

            # Calculate the moment of inertia tensor using the formula from Quadrotor Dynamics and Control by Randal Beard
            ixx = ((2 * self.quads['weight'] * self.quads['r'] ** 2) / 5) + (
                        2 * self.quads['weight'] * self.quads['L'] ** 2)
            iyy = ixx
            izz = ((2 * self.quads['weight'] * self.quads['r'] ** 2) / 5) + (
                        4 * self.quads['weight'] * self.quads['L'] ** 2)
            self.I = torch.tensor([[ixx, 0, 0], [0, iyy, 0], [0, 0, izz]], dtype=torch.float64)

            # Calculate the inverse of the moment of inertia tensor
            self.invI = torch.inverse(self.I)
            self.count = 0
        def rotation_matrix(self, angles):
            # Calculate the cosine of each angle
            ct = torch.cos(angles[0])
            cp = torch.cos(angles[1])
            cg = torch.cos(angles[2])
            # Calculate the sine of each angle
            st = torch.sin(angles[0])
            sp = torch.sin(angles[1])
            sg = torch.sin(angles[2])
            # Define rotation matrices along each axis
            R_x = torch.tensor([[1, 0, 0], [0, ct, -st], [0, st, ct]])
            R_y = torch.tensor([[cp, 0, sp], [0, 1, 0], [-sp, 0, cp]])
            R_z = torch.tensor([[cg, -sg, 0], [sg, cg, 0], [0, 0, 1]])
            # Calculate the full rotation matrix by composing the rotations along each axis
            R = torch.matmul(torch.matmul(R_z, R_y), R_x)
            return R

        def wrap_angle(self, val):
            return ((val + np.pi) % (2 * np.pi) - np.pi)

        def forward(self, state, action, t=0.02):
            """
               This function performs the forward dynamics calculation of a quadcopter given the state variables and actions.

               Parameters:
               state (List[float]): a list of 12 state variables [x_pos, y_pos, z_pos, x_acc, y_acc, z_acc, roll , pitch, yaw, roll_acc , pitch_acc, yaw_acc].
               action (List[float]): a list of 4 motor throttle percentages [prop1 , prop2, prop3, prop4], with speed values ranging from 0 to 1.
               t (float): time step, with a default value of 0.02 seconds.

               Returns:
               state_dot (np.array): an array of 12 state derivatives, representing the updated state of the quadcopter.
           """


            self.count+=1
            print("Quad forward call :" + str(self.count))

            state.requires_grad = True
            action.requires_grad = True

            action = action[0]
            if len(state) == 1:
                state = state[0]

            # print("state " + str(state))
            # print("action " + str(action))
            # Calculate the speed of each rotor by scaling the action values with the maximum RPM
            rotor1Speed = action[0] * self.maxRPM
            rotor2Speed = action[1] * self.maxRPM
            rotor3Speed = action[2] * self.maxRPM
            rotor4Speed = action[3] * self.maxRPM

            # Set the speed of each rotor using the Prop1-Prop4 objects
            self.Prop1.set_speed(rotor1Speed)
            self.Prop2.set_speed(rotor2Speed)
            self.Prop3.set_speed(rotor3Speed)
            self.Prop4.set_speed(rotor4Speed)

            # Initialize the state_dot array to store the derivatives of the state
            state_dot = torch.zeros(12)

            # Velocity equations: the velocity in each dimension (x, y, z) remains constant
            state_dot[0] = state[3]
            state_dot[1] = state[4]
            state_dot[2] = state[5]

            # The acceleration equations
            # Calculate the net force acting on the quadcopter using thrust, gravity, and drag
            # x_dotdot = np.array([0, 0, -self.quads['weight'] * self.g]) \
            #            + np.dot(self.rotation_matrix(state[6:9]),
            #                     np.array([0, 0, (self.Prop1.thrust +
            #                                      self.Prop2.thrust +
            #                                      self.Prop3.thrust +
            #                                      self.Prop4.thrust)])) \
            #            / self.quads['weight']
            x_dotdot = torch.cat((torch.tensor([0., 0., -self.quads['weight'] * self.g]),
                                  torch.div(
                                      torch.matmul(
                                          self.rotation_matrix(state[6:9]),
                                          torch.tensor(
                                              [0., 0., self.Prop1.thrust + self.Prop2.thrust +
                                               self.Prop3.thrust + self.Prop4.thrust])
                                      ),
                                      self.quads['weight'])))
            # Update the acceleration in each dimension (x, y, z)
            state_dot[3] = x_dotdot[0]
            state_dot[4] = x_dotdot[1]
            state_dot[5] = x_dotdot[2]

            # Angular rate equations: the angular rate in each dimension (roll, pitch, yaw) remains constant
            state_dot[6] = state[9]
            state_dot[7] = state[10]
            state_dot[8] = state[11]
            # The angular accelerations
            omega = state[9:12]
            tau = torch.tensor([self.quads['L'] *
                            (self.Prop1.thrust -
                             self.Prop3.thrust), self.quads['L'] *
                            (self.Prop2.thrust -
                             self.Prop4.thrust), self.b *
                            (self.Prop1.thrust -
                             self.Prop2.thrust +
                             self.Prop3.thrust -
                             self.Prop4.thrust)])

            omega_dot = torch.mul(self.invI,
                                  tau - torch.cross(
                                      omega, torch.matmul(
                                          self.I, omega)))
            state_dot[9] = omega_dot[0][0]
            state_dot[10] = omega_dot[1][1]
            state_dot[11] = omega_dot[2][2]
            # state_dot[9] = omega_dot[0]
            # state_dot[10] = omega_dot[1]
            # state_dot[11] = omega_dot[2]
            #step proportional to time step t
            state_dot = torch.zeros(12)
            new_state = state + state_dot * t

            #FIX ROTATIONS AND GROUND
            new_state[6:9] = self.wrap_angle(new_state[6:9])
            # print("new state "+ str(new_state))
            if float(new_state[2]) < 0:
                new_state[2] = 0
                new_state[5] = 0


            return new_state.reshape(1,12)

    def __init__(self,
                 env_func=None,
                 NUM_STATES=12,  # number of states
                 NUM_ACTIONS=4,  # number of actions
                 ACTION_LOWS=[0.0, 0.0, 0.0, 0.0],  # lower bound of actions
                 ACTION_HIGHS=[1.0, 1.0, 1.0, 1.0],  # upper bound of actions
                 TIMESTEPS=20,  # number of time steps in the horizon
                 LQR_ITER=10,  # number of LQR iterations
                 N_BATCH=12,  # batch size
                 RENDER=False,  # render the environment or not
                 **kwargs
                 ):
        """
        Initialization for Model Predictive Controller class.

        Parameters:
        env_func (callable): A function that returns an environment.
        NUM_STATES (int): Number of states.
        NUM_ACTIONS (int): Number of actions.
        ACTION_LOWS (np.ndarray): Lower bound of actions.
        ACTION_HIGHS (np.ndarray): Upper bound of actions.
        TIMESTEPS (int): Number of time steps in the horizon.
        LQR_ITER (int): Number of LQR iterations.
        N_BATCH (int): Batch size.
        RENDER (bool): Whether to render the environment or not.
        kwargs (dict): Additional keyword arguments.
        """
        super().__init__(env_func, **kwargs)
        self.env = env_func()  # initialize the environment

        # set device as cuda if available, otherwise use cpu
        self.device = torch.device(
            "cuda:0" if torch.cuda.is_available() else "cpu")
        self.dtype = torch.float64  # data type for tensors

        self.ctrl_penalty = 0.001  # penalty for control inputs

        # initialize the quadrotor dynamics model
        self.model = self.QuadDynamics()

        self.num_states = NUM_STATES  # number of states
        self.num_actions = NUM_ACTIONS # number of actions
        self.horizon = TIMESTEPS  # number of time steps in the horizon
        self.lqr_iter = LQR_ITER  # number of LQR iterations
        self.n_batches = N_BATCH # batch size
        self.actionLows = torch.tensor(ACTION_LOWS)  # lower bound of actions
        self.actionHighs = torch.tensor(ACTION_HIGHS) # upper bound of actions
        self.goal = torch.zeros(self.num_states,  dtype=torch.float64)
        torch.reshape(self.goal, (12, 1))
        self.goal[2] = 5  # set the goal height to 5
        self.render = RENDER #  not

    def cost_function(self):
        # Define goal weights (penalty coefficients) for each state
        goal_weights = torch.ones(self.num_states)  # equal weights
        # Define the goal state as a tensor
        goal_state = torch.tensor(self.goal, dtype=self.dtype)  # nx

        # Concatenate the goal weights and the control penalty
        q = torch.cat((
            goal_weights,
            self.ctrl_penalty * torch.ones(self.num_actions, dtype=self.dtype)
        ))  # nx + nu
        # Calculate px as negative square root of goal weights times the goal state
        px = -torch.sqrt(goal_weights) * goal_state
        # Concatenate px and zeros for the actions
        p = torch.cat((px, torch.zeros(self.num_actions, dtype=self.dtype)))
        # Repeat the diagonal of q and p to match the horizon and batch size
        Q = torch.diag(q).repeat(self.horizon, self.n_batches, 1, 1)  # T x B x nx+nu x nx+nu
        p = p.repeat(self.horizon, self.n_batches, 1)
        # Define a QuadCost object with Q and p
        cost = mpc.QuadCost(Q, p)
        # Return the cost object
        return cost

    def run(self, n_episodes=1000):
        """Run the MPC controller for n_episodes.

        Args:
        - n_episodes (int): number of episodes for which the MPC controller is to be run.

        Returns:
        None

        """
        # Set initial values for the total reward and action
        self.total_reward = 0
        self.action = torch.zeros(self.num_actions)
        u_init = None

        # Loop over the number of episodes
        for i in range(n_episodes):
            # Get the current state of the environment
            self.state = torch.tensor(self.env.get_state().copy()).unsqueeze(0)

            # Create a new instance of the MPC controller
            ctrl = mpc.MPC(
                self.num_states,
                self.num_actions,
                self.horizon,
                u_lower=self.actionLows,
                u_upper=self.actionHighs,
                lqr_iter=self.lqr_iter,
                exit_unconverged=False,
                eps=1e-2,
                n_batch=self.n_batches,
                backprop=False,
                verbose=-1,
                u_init=u_init,
                grad_method=mpc.GradMethods.AUTO_DIFF)

            # print(self.state.ndimension())
            # torch.reshape(self.state, (12,1 ))
            # print(self.state.ndimension())
            # print(self.state.ndimension()==2)
            # print(self.state.size(0))
            # print(self.n_batches)
            # print(self.state.size(0)==self.n_batches)
            print("Pre LQR projection :" + str(i))
            # Calculate the nominal states, actions and objectives based on the current state, cost function, and dynamics model
            nominal_states, nominal_actions, nominal_objs = ctrl(
                self.state, self.cost_function(), self.model)

            print("Completed LQR projection :" + str(i))
            # Take the first planned action as the current action
            self.action = nominal_actions[0]
            print("Executing Action :" + str(self.action ))
            # Create the input sequence for the next iteration by concatenating the planned actions starting from the second action
            # with zeros for the remaining actions
            u_init = torch.cat((nominal_actions[1:], torch.zeros(
                1, self.n_batches, self.num_actions, dtype=self.dtype)), dim=0)

            # Update the state of the environment based on the action taken and obtain the reward
            s, r, _, _ = self.env.step(self.action.detach().numpy())

            # Update the total reward
            self.total_reward += r

            # Render the environment if render flag is set to True
            if self.render:
                self.env.render()


    def reset(self):

        return
